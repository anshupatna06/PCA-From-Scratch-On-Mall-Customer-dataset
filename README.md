# PCA-From-Scratch-On-Mall-Customer-dataset
"ML models implemented from scratch using NumPy and Pandas only"

# ğŸ§  Principal Component Analysis (PCA) â€” From Scratch

## ğŸ“˜ Overview
This project demonstrates **Principal Component Analysis (PCA)** implemented **from scratch** using only NumPy and visualized with Matplotlib and Seaborn.  
PCA is an **unsupervised dimensionality reduction** technique used to identify directions (principal components) that capture the **maximum variance** in data.

---

## âš™ï¸ Workflow

1. **ğŸ“š Import Libraries:** `numpy`, `pandas`, `matplotlib`, `seaborn`, `sklearn`
2. **ğŸ§¼ Standardize Data:** Scale features to zero mean and unit variance.
3. **ğŸ“Š Compute Covariance Matrix:**  
   $$\[\text{Cov}(Z) = \frac{1}{n-1} Z^T Z\]$$
4. **ğŸ§® Find Eigenvalues & Eigenvectors:**  
   $$\[\text{Cov}(Z)v = \lambda v\]$$
5. **ğŸ“ˆ Sort Eigenvalues (Descending):** Select top `k` eigenvectors â†’ principal components.
6. **ğŸ’¡ Project Data:**  
   $$\[Z_{\text{reduced}} = Z \cdot W\]$$
   where `W` is the matrix of top eigenvectors.
7. **ğŸ¨ Visualize Results:**  
   - Data projected on new axes  
   - Cumulative explained variance plot

---

## ğŸ§® Mathematical Concepts

| Concept | Formula / Description |
|----------|------------------------|
| **Standardization** | $$\( Z = \frac{X - \mu}{\sigma} \)$$ |
| **Covariance Matrix** | $$\( \text{Cov}(Z) = \frac{1}{n-1} Z^T Z \)$$ |
| **Eigen Decomposition** | $$\( \text{Cov}(Z)v = \lambda v \)$$ |
| **Explained Variance Ratio** | $$\( r_i = \frac{\lambda_i}{\sum_j \lambda_j} \)$$ |
| **Projection** | $$\( Z_{\text{reduced}} = Z \cdot W \)$$ |

---

## ğŸ“Š Visualization Outputs

1. **Covariance Matrix**  
   Displays inter-feature relationships.

2. **Projected Data (2D Scatter Plot)**

Shows data in principal component space.

3. **Cumulative Explained Variance Plot**  
Illustrates how much information is retained with each component.

---

## ğŸ§© Dataset
**Mall_Customers.csv**  
Features used:
- `Annual Income (k$)`  
- `Spending Score (1â€“100)`

---

## ğŸ’» Code Summary
```python
# 1ï¸âƒ£ Standardize
X_scaled = (X - X.mean(axis=0)) / X.std(axis=0)

# 2ï¸âƒ£ Covariance
cov_matrix = np.cov(X_scaled.T)

# 3ï¸âƒ£ Eigen Decomposition
eig_vals, eig_vecs = np.linalg.eig(cov_matrix)

# 4ï¸âƒ£ Sort & Project
sorted_idx = np.argsort(eig_vals)[::-1]
W = eig_vecs[:, sorted_idx[:2]]
X_pca = np.dot(X_scaled, W)


ğŸ“ˆ Results

PCA successfully reduced data to 2 principal components.

The first component captured most variance (customer purchasing power).

The second captured smaller localized spending variations.

Compared results matched perfectly with scikit-learn PCA output.



---

ğŸš€ Future Improvements:-

Apply PCA on higher-dimensional datasets (e.g., image or text embeddings).

Combine PCA with clustering algorithms (KMeans, DBSCAN) for hybrid insights.

Extend to Kernel PCA for non-linear transformations.



---

ğŸ·ï¸ Author

Anshu Pandey
ğŸ“Š From Scratch Implementation with Mathematical Insights & Visualizations


---

â­ References

Scikit-learn PCA Documentation

Mall Customer Dataset (Kaggle)
